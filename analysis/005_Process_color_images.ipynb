{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE A PIPELINE FOR PROCESSING THE COLOR IMAGES!\n",
    "# FROM TIRAMISU\n",
    "# IDEA: Add neck to the posture map?\n",
    "# %matplotlib inline\n",
    "# %matplotlib widget\n",
    "%matplotlib qt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import sys, os, pickle\n",
    "import cv2\n",
    "from colour import Color\n",
    "import h5py\n",
    "from tqdm import tqdm, tqdm_notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GeForce RTX 2080 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "torch_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the hourglass net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the hourglass model and set up architecture\n",
    "from architectures.hourglass import hg\n",
    "global best_acc\n",
    "\n",
    "model = hg(\n",
    "    num_stacks=8,\n",
    "    num_blocks=1,\n",
    "    num_classes=11,\n",
    "    num_feats=128,\n",
    "    inplanes=64,\n",
    "    init_stride=2,\n",
    ")\n",
    "    \n",
    "model = torch.nn.DataParallel(model).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load well-trained weights and apply to the hourglass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights '192x320_weights/singlecore_weights_epoch_119_loss_97.685_2020-08-20_13-15-51.pth'\n",
      "loaded!\n"
     ]
    }
   ],
   "source": [
    "# A HELPER FUNCTION WHICH SAVES THE STATE OF THE NETWORK, maybe every 10 epochs or smth?\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "epoch = 50\n",
    "epoch = 110\n",
    "\n",
    "# save_weights(model, 0, 1000)    \n",
    "# def load_weights(model, epoch):\n",
    "WEIGHTS_PATH = '192x320_weights/'\n",
    "\n",
    "# # the the most recent from that epoch\n",
    "# all_options = sorted( glob.glob(WEIGHTS_PATH + '/singlecore_weights_epoch_'+str(epoch)+'*' ) )\n",
    "# print(all_options)\n",
    "# weights_fpath = all_options[0]\n",
    "\n",
    "# CHOSEN\n",
    "# weights_fpath = WEIGHTS_PATH + 'singlecore_weights_epoch_135_loss_1565.910_2019-11-22_11-14-39.pth'\n",
    "weights_fpath = WEIGHTS_PATH + 'singlecore_weights_epoch_119_loss_97.685_2020-08-20_13-15-51.pth'\n",
    "\n",
    "print(\"loading weights '{}'\".format(weights_fpath))\n",
    "model.load_state_dict( torch.load(weights_fpath) )\n",
    "model.eval()\n",
    "print('loaded!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap images in pytorch dataset and make helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a stacvk of some sorted frames!\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "# from deepposekit import VideoReader, KMeansSampler\n",
    "import sys\n",
    "# sys.path.append('/home/chrelli/git/3d_sandbox/mouseposev0p1/chrelli_annotator/')\n",
    "# sys.path.append('/home/chrelli/git/3d_sandbox/mousepose_0p1/deepposekit-annotator/')\n",
    "\n",
    "# from dpk_annotator import VideoReader, KMeansSampler\n",
    "import tqdm\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['start_frame', 'end_frame', 'd_cam_params', 'c_cam_params', 'R_extrinsics', 't_extrinsics', 'R_world', 't_world', 'M0', 'floor_point', 'floor_normal', 'c_cylinder', 'r_cylinder'])\n",
      "dict_keys(['master_frame_table', 'reference_time_cam', 'reference_stamps', 'time_stamps', 'shifted_stamps'])\n"
     ]
    }
   ],
   "source": [
    "# load the geometry\n",
    "top_folder_0 = '/media/chrelli/Data0/recording_20190905-115115'\n",
    "top_folder_1 = '/media/chrelli/Data1/recording_20190905-115115'\n",
    "\n",
    "top_folder_0 = '/media/chrelli/Data0/recording_20200806-171004'\n",
    "top_folder_1 = '/media/chrelli/Data1/recording_20200806-171004'\n",
    "\n",
    "scene_folders = [top_folder_0,top_folder_0,top_folder_1,top_folder_1]\n",
    "import pickle\n",
    "geometry = pickle.load( open( scene_folders[0]+'/geometry.pkl', \"rb\" ) ) \n",
    "timing = pickle.load( open( scene_folders[0]+'/timing.pkl', \"rb\" ) )\n",
    "print(geometry.keys())\n",
    "print(timing.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also make a list of all the frames to process\n",
    "# make a list of the cameras!\n",
    "png_files = [glob.glob(scene_folders[i] + '/npy_raw/dev' +str(i) +'_cad_*.png') for i in range(4)]\n",
    "png_files = [sorted(f) for f in png_files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRAP inside of a pytorch dataset\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from c_utils.utils_hour import gaussian\n",
    "\n",
    "class ReadDataset(data.Dataset):\n",
    "    # todo add augmentation here, clean up and make faster\n",
    "    # todo remove stupid side effects etc\n",
    "    def __init__(self, dev,png_files):\n",
    "        '''Initialization'''\n",
    "        self.dev = dev\n",
    "        self.file_list = png_files[dev]\n",
    "        self.n_frames = len(self.file_list)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.n_frames    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # returns the image in RBG\n",
    "        c_image = cv2.imread(self.file_list[index])\n",
    "        # REMEMBER TO CUT DOWN THE TOP of the revolution, such that the image is 192x320\n",
    "        # pack depth and pixels to target - OR NOT??\n",
    "        im = c_image\n",
    "        frame_height = im.shape[0]\n",
    "        frame_width = im.shape[1]\n",
    "        \n",
    "        # make the resolution correct, i.e. set the height to 192\n",
    "        pad_top = 8\n",
    "        pad_bottom = 10\n",
    "        im = im[pad_top:-pad_bottom,:,:]       \n",
    "        # return the index AND flip from rgb to bgr AND normalize to 0 to 1\n",
    "#         return index, np.moveaxis( im[:,:,[2,1,0]], 2, 0)\n",
    "        return index, np.moveaxis( im, 2, 0)\n",
    "\n",
    "    \n",
    "# # we shuffle, so that we always see different dumps\n",
    "dev = 0\n",
    "FrameLoader = data.DataLoader( ReadDataset(dev,png_files) , batch_size=12, shuffle=False, num_workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3, 192, 320])\n",
      "torch.uint8\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor(255, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# geneate some frames, show as video to see that it's working!\n",
    "for _ in range(1):    \n",
    "#     im,target = random_from( MouseTrainLoader)\n",
    "    index,im_batch = next(iter( FrameLoader))\n",
    "    print(im_batch.shape)\n",
    "    print(im_batch.dtype)\n",
    "    print(index)\n",
    "    print(torch.max(im_batch))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for i in range(12):\n",
    "        plt.subplot(3,4,1+i)\n",
    "#         plt.imshow(np.moveaxis(im_batch[i,...].numpy(),0,2))\n",
    "        plt.imshow(np.moveaxis(im_batch[i,...].numpy(),0,2))\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('matplotlib wants rgb, network wants bgr \\n verify here, i.e. \\n these colors must look WEIRD')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/pre_color_01.png\" width = 50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the dataset and generate the score maps!\n",
    "\n",
    "# function to pass through the network\n",
    "def im_batch_2_scores(im_batch,model):\n",
    "    inputs = im_batch.float().div(255.).cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # compute model output\n",
    "        output = model(inputs)\n",
    "        # get the resulting scores out! Drop the affinity maps\n",
    "        scores = output[-1][:,:4,:,:]\n",
    "    return scores.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some scores to see that it's fine\n",
    "sco = im_batch_2_scores(im_batch,model)\n",
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "for i in range(12):\n",
    "    plt.subplot(3,4,1+i)\n",
    "    plt.imshow(sco[i,1,:,:])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Example network output')\n",
    "plt.subplots_adjust(hspace=.1,wspace=.1)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/pre_color_02.png\" width = 50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def single_score_2_keypoints(sco):\n",
    "    xy_list = [None]*4\n",
    "    pxy_list = [None]*4\n",
    "    score_idx_list = [None]*4\n",
    "    for key in range(4):\n",
    "        xy = peak_local_max(sco[key,:,:],threshold_abs = 0.5,num_peaks = 6)\n",
    "        xy_list[key] = xy\n",
    "        pxy_list[key] = sco[key,xy[:,0],xy[:,1]]\n",
    "        # print(xy.shape)\n",
    "        score_idx_list[key] = key * np.ones_like(xy)\n",
    "\n",
    "    return np.concatenate(xy_list), np.concatenate(pxy_list), np.concatenate(score_idx_list)\n",
    "\n",
    "def save_keypoints(dev,index,sco,hf_file):\n",
    "    for i in range(len(index)):\n",
    "        xy, pxy, score_idx = single_score_2_keypoints(sco[i,...])\n",
    "        # stack everythong together in a list\n",
    "        data_string = np.hstack( [xy.ravel(), np.round(pxy*100).astype('int') ,score_idx[:,0]])\n",
    "        hf_file['dataset'][index[i]] = data_string\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass all images through the net and save the detected keypoints in hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting /media/chrelli/Data0/recording_20200806-171004/keypoints_0.hdf5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dcac9c04f34d55972b9980efca4a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=504), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/media/chrelli/Data0/recording_20200806-171004/keypoints_0.hdf5 is done!\n",
      "starting /media/chrelli/Data0/recording_20200806-171004/keypoints_1.hdf5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8463d9e02a49738ded80613e11c476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/media/chrelli/Data0/recording_20200806-171004/keypoints_1.hdf5 is done!\n",
      "starting /media/chrelli/Data0/recording_20200806-171004/keypoints_2.hdf5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65de01e602474fde9e8c47fc8708b5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/media/chrelli/Data0/recording_20200806-171004/keypoints_2.hdf5 is done!\n",
      "starting /media/chrelli/Data0/recording_20200806-171004/keypoints_3.hdf5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2e6d71bdfb413b93eac5a74400990f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/media/chrelli/Data0/recording_20200806-171004/keypoints_3.hdf5 is done!\n"
     ]
    }
   ],
   "source": [
    "# from tqdm.auto import tqdm # to automatically use tqdm notebook\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for dev in range(4):\n",
    "# for dev in [1,2,3]:\n",
    "    # make the file, if it does not already exist\n",
    "    hdf5_file_path = top_folder_0+'/keypoints_'+str(dev)+'.hdf5'\n",
    "    if os.path.exists(hdf5_file_path):\n",
    "        print(hdf5_file_path+\" was already done!\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"starting \"+hdf5_file_path+\"...\")\n",
    "        \n",
    "    # make a frame loader\n",
    "    FrameLoader = data.DataLoader( ReadDataset(dev,png_files) , batch_size=50, \n",
    "                                  shuffle=False, num_workers = 10)\n",
    "    \n",
    "    # open the file\n",
    "    n_pictures = len(png_files[dev])\n",
    "    with h5py.File(hdf5_file_path, mode='w') as hdf5_file:\n",
    "        # make the variable length dataset\n",
    "        dt = h5py.special_dtype(vlen=np.dtype('int'))\n",
    "        hdf5_file.create_dataset('dataset', (n_pictures,), dtype=dt)\n",
    "        # save the keypoint to an h5py file by looping over!\n",
    "        for index,im_batch in tqdm_notebook(FrameLoader):\n",
    "            sco = im_batch_2_scores(im_batch,model)\n",
    "            save_keypoints(dev,index,sco,hdf5_file)    \n",
    "    \n",
    "    print(hdf5_file_path+\" is done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['dataset']>\n",
      "24980\n",
      "[ 26  56  24  43  29  57  25  41  27  49  90 127  96  57  84   1   1   2\n",
      "   2   3]\n"
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    # Wuick check to see if the data is in the h5py file\n",
    "    with h5py.File(top_folder_0+'/keypoints_'+str(dev)+'.hdf5', mode='r') as hdf5_file:\n",
    "        print(hdf5_file.keys())\n",
    "        print(len(hdf5_file['dataset']))\n",
    "        print( hdf5_file['dataset'][500] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
